---
title: "A Study on the Zero-Modified Poisson Model"
author: "Marcelo Ventura Freire"
format: 
  # pdf:
  #   toc: true
  #   number-sections: true
  html:
    embed-resources: true
    toc: true
    toc-depth: 4
    number-sections: true
execute: 
  echo: false
  cache: true
---

```{r}
#| label: setup
#| include: false

library(tidyverse)  # my go-to package
library(actuar)     # dzmpois() 
library(latex2exp)  # TeX()
library(patchwork)
library(lamW)

`%$%` <- paste0  # concatenação de string
```


<!-- ----------------------------------------------------------------------- -->

## Introduction

I did not invented this model, but I will give the proper credits later, when I write a proper introduction.

### Literature review

Soon...

### Examples of applications

TO-DO: examples of phenomena with occurrences of over- and under-occurrences of zero observations.


<!-- ----------------------------------------------------------------------- -->

## Definitions

From the (regular) Poisson distribution $X \sim Poi(\lambda)$ with $\lambda\in(0,\infty)$ and $x=0,1,2,\ldots$
$$
P(X=x) = \frac{e^{-\lambda}\lambda^x}{x!}
$$
and $p_0=P(X=0)=e^{-\lambda}$, we define:

- the *zero-truncated* Poisson distribution $Y \sim ZTruncPoi(\lambda)$, with 
  $\lambda\in(0,\infty)$
  $$
  P(Y=y) 
    = P(X=y|X>0) 
    = \frac{P(X=y)}{1-P(X=0)}
    % = \frac{\frac{e^{-\lambda}\lambda^y}{y!}}{(1-e^{-\lambda})}
    = \frac{\lambda^y}{(e^\lambda-1)y!},
  $$
  for $y=1,2,\ldots$
- the *zero-inflated* Poisson distribution $Z \sim ZInflPoi(\pi, \lambda)$, 
  with $\pi\in(0,1)$ and $\lambda\in(0,\infty)$
  $$
  P(Z=z) =
    \begin{cases}
    \pi + (1-\pi) \cdot P(X=0) 
      = \pi + (1-\pi)e^{-\lambda}
      & z=0\\
    (1-\pi) P(X=z) 
      = \frac{(1-\pi)e^{-\lambda}\lambda^z}{z!},
      & z=1,2,\ldots
    \end{cases}
  $$
  which can be thought of as a mixture model as 
  $Z = I \times 0 + (1 - I) \times X$
  where $I \sim Bern(\pi)$ independent of $X$.
- the (general) *zero-modified* Poisson distribution
  $W \sim ZModPoi(p, \lambda)$, with $p\in(0,1)$ and $\lambda>0$
  $$
  P(W=w) =
    \begin{cases}
    p & w=0\\
    (1-p) \cdot P(Y=w) 
      = \frac{(1-p)\lambda^w}{(e^\lambda-1)w!},
      & w=1,2,\ldots
    \end{cases}
  $$
  which can be thought of as a hurdle model as 
  $W = J \times 0 + (1 - J) \times Y$
  where $J \sim Bern(p)$ independent of $Y$.


Note that the regular, the zero-inflated, the zero-deflated, and the zero-truncated Poisson distributions can be obtained from the general model respectively when $p=p_0$, $p>p_0$, $p<p_0$, and $p\to0$.

```{r}
#| label: fig-1st-parametrization
#| fig-cap: "Visualizing difference among distributions"

xfinal <- 6
expand_grid(
  x = 0:xfinal,
  lambda = c(0.5, 1, 2, 3)
) |> 
  mutate(
    RegPoi =  
      dzmpois(x, lambda, dpois(0, lambda)),
    ZTruncPoi =
      dzmpois(x, lambda, 0),
    ZDeflPoi = 
      dzmpois(x, lambda, 0.5 * dpois(0, lambda)),
    ZInfPoi = 
      dzmpois(x, lambda, 1.5 * dpois(0, lambda)),
  ) |> 
  pivot_longer(
    cols = RegPoi:ZInfPoi,
    values_to = "y",
    names_to = "distrib",
  ) |> 
  ggplot() +
  aes(x = x, y = y, group = distrib, color = distrib) +
  geom_point() +
  geom_line() +
  scale_x_continuous(
    breaks = 0:xfinal,
    labels = scales::label_number(accuracy = 1)
  ) +
  ylim(0:1) +
  facet_wrap(~ lambda, labeller = "label_both") +
  ggtitle( 
    "Comparing regular, zero-truncated, zero-deflated, and zero-inflated models" ,
    subtitle = 
      "Regular ($p=p_0$), zero-truncated ($p=0$), " %$%
      "zero-deflated ($p<p_0$), and zero-inflated ($p>p_0$)" |> 
      TeX()
  ) +
  xlab("z") +
  ylab("P(Z=z)") +
  theme_classic()
```


A second parametrization for the zero-modified Poisson distribution can be obtained by setting $p=e^{-\lambda+\theta}$, so that, for $Z \sim ZModPoi(\theta, \lambda)$, we have
$$
P(W=w) = 
\begin{cases}
e^{-\lambda+\theta}, 
  & w=0\\
\frac{(1-e^{-\lambda+\theta})\lambda^w}{(e^{\lambda}-1)w!}
  & w=1,2,\ldots
\end{cases}
$$
in which case, regular, zero-inflated, and zero-deflated Poisson can be obtained when $\theta=0$, $\theta>0$, and $\theta<0$. However, a constraint must be set on $\theta$ so that $0<p<1$:
$$
e^{-\lambda+\theta}<1
  \Leftrightarrow
    -\lambda+\theta<0
  \Leftrightarrow
    \theta<\lambda.
$$
This way, we can refer to $Z \sim ZModPoi(\theta,\lambda)$ for $\lambda>0$ and $\theta<\lambda$ as the model in its second parametrization.


For algebraic ease, we may use a third parametrization setting $1-p=e^\gamma$, subject to $0<e^\gamma<1 \Leftrightarrow \gamma<0$, 
$$
P(W=w) = 
\begin{cases}
1-e^{\gamma}, 
  & y=0\\
\frac{e^\gamma\lambda^w}{(e^{\lambda}-1)w!}
  & w=1,2,\ldots
\end{cases}
$$
so that the parametrization used in each context will be defined by the use of the three different parameters.



<!-- ----------------------------------------------------------------------- -->

## The regular Poisson model

### Moments of the distribution

The first to moments $\mu_1(\lambda)$ and $\mu_2(\lambda)$ are
$$
\begin{aligned}
\mu_1(\lambda) 
  & = \mathbb{E}(X) \\
  & = \sum_{x=0}^\infty x \frac{e^{-\lambda}\lambda^x}{x!}   
    = \sum_{x=1}^\infty x \frac{e^{-\lambda}\lambda^x}{x!}   
    = \sum_{x=1}^\infty \frac{e^{-\lambda}\lambda^{x-1}\lambda}{(x-1)!} \\
  & = \lambda \sum_{x=1}^\infty \frac{e^{-\lambda}\lambda^{x-1}}{(x-1)!}   
    = \lambda \sum_{x=0}^\infty \frac{e^{-\lambda}\lambda^x}{x!}   
    = \lambda \\
\mu_2(\lambda) 
  & = \mathbb{E}(X^2) \\
  & = \sum_{x=0}^\infty x^2 \cdot P(X=k)   
    = \sum_{x=0}^\infty x^2 \frac{e^{-\lambda}\lambda^x}{x!}   
    = \sum_{x=1}^\infty x^2 \frac{e^{-\lambda}\lambda^x}{x!} \\
  & = \sum_{x=1}^\infty x \frac{e^{-\lambda}\lambda^{x-1}\lambda}{(x-1)!}   
    = \lambda\sum_{x=1}^\infty (x - 1 + 1) \frac{e^{-\lambda}\lambda^{x-1}}{(x-1)!} \\
  & = \lambda\sum_{x=1}^\infty (x - 1) \frac{e^{-\lambda}\lambda^{x-1}}{(x-1)!}
      + \lambda\sum_{x=1}^\infty \frac{e^{-\lambda}\lambda^{x-1}}{(x-1)!} \\
  & = \lambda\sum_{x=2}^\infty (x - 1) \frac{e^{-\lambda}\lambda^{x-1}}{(x-1)!}
      + \lambda\sum_{x=0}^\infty \frac{e^{-\lambda}\lambda^{x}}{x!} \\
  & = \lambda\sum_{x=2}^\infty \frac{e^{-\lambda}\lambda^{x-2}\lambda}{(x-2)!}
      + \lambda   
    = \lambda^2\sum_{x=0}^\infty \frac{e^{-\lambda}\lambda^{x}}{x!} + \lambda   
    = \lambda^2 + \lambda \\
\end{aligned}
$$
since $\sum_{x=0}^\infty \lambda^x/x! = e^\lambda$ and $\sum_{x=0}^\infty e^{-\lambda}\lambda^x/x! = e^{-\lambda}\sum_{x=0}^\infty \lambda^x/x! = e^{-\lambda}e^{\lambda}=1$.

The variance $Var(X)$ is
$$
\begin{aligned}
Var(X) 
  & = \mu_2(\lambda) - \mu_1(\lambda)^2
    = \lambda^2 + \lambda - \lambda^2
    = \lambda
\end{aligned}
$$



### Log-likelihood of the distribution and related functions

The log-likelihood function $\ell(\lambda|x)$, the score function $U(\lambda|x)$, observed information $\mathcal{J}(\lambda|x)$, and Fisher information $\mathcal{I}_X(\lambda)$ of a single observation $X \sim Poi(\lambda)$ are
$$
\begin{aligned}
\ell(\lambda|x) 
  & = \log(P(X=x)) 
  % & = \log(P(X=x)) \\
  % & = -\lambda + x \log(\lambda) \\
  % & = x \log(\lambda) - \lambda \\
  = x \log(\lambda) - \lambda \\
%
U(\lambda|x) 
  & = \ell'(\lambda|x)
  % & = \ell'(\lambda|x) \\
  % & = \frac{x}{\lambda} - 1 \\
  = \frac{x}{\lambda} - 1 \\
%
\mathcal{J}(\lambda|x) 
  & = - U'(\lambda|x) 
  % & = - U'(\lambda|x) \\
  % & = - \ell''(\lambda|x) \\
  % & = \frac{x}{\lambda^2} \\
  = \frac{x}{\lambda^2} \\
%
\mathcal{I}_X(\lambda) 
  & = \mathbb{E}_X(\mathcal{J}(\lambda|X)) 
  % & = \mathbb{E}_X(\mathcal{J}(\lambda|X)) \\
  % & = \mathbb{E}\left(\frac{X}{\lambda^2}\right) \\
  % & = \frac{\lambda}{\lambda^2} \\
  % & = \frac{1}{\lambda}
  = \frac{1}{\lambda}
\end{aligned}
$$
unless a term constant with respect to $\lambda$.


### Maximum likelihood estimation

For an i.i.d. sample $\widetilde{X}=(X_1,\ldots,X_n)$, the log-likelihood function $\ell(\lambda|\widetilde{x})$, the score function $U(\lambda|\widetilde{x})$, observed information $\mathcal{J}(\lambda|\widetilde{x})$, and Fisher information $\mathcal{I}_{\widetilde{X}}(\lambda)$ are
$$
\begin{aligned}
\ell(\lambda|\widetilde{x}) 
  & = \sum_{i=1}^n \ell(\lambda|x_i)
  % & = \sum_{i=1}^n \ell(\lambda|x_i) \\
  % & = \sum_{i=1}^n -\lambda + x_i \log(\lambda) 
  % & = - n \lambda + \log(\lambda) \sum_{i=1}^n x_i
  % & = - n \lambda + s \log(\lambda)
  % & = - n \lambda + n\bar{x} \log(\lambda) \\
  % & = n \bar{x} \log(\lambda) - n \lambda \\
  = n \bar{x} \log(\lambda) - n \lambda \\
%
U(\lambda|\widetilde{x}) 
  & = \ell'(\lambda|\widetilde{x}) 
  % & = \ell'(\lambda|\widetilde{x}) \\
  % & = \frac{n\bar{x}}{\lambda} - n \\
    = \frac{n\bar{x}}{\lambda} - n \\
%
\mathcal{J}(\lambda|\widetilde{x}) 
  & = - U'(\lambda|\widetilde{x}) 
  % & = - U'(\lambda|\widetilde{x}) \\
  % &  = \frac{n\bar{x}}{\lambda^2} \\
  = \frac{n\bar{x}}{\lambda^2} \\
%
\mathcal{I}_{\widetilde{X}}(\lambda) 
  & = n \cdot \mathcal{I}_X(\lambda) 
  % & = n \cdot \mathcal{I}_X(\lambda)  \\
  % & = \frac{n}{\lambda} \\
    = \frac{n}{\lambda}
\end{aligned}
$$
by the additivity of the likelihood function of i.i.d. samples.

Note that, in the worst case scenario $\widetilde{x}=\widetilde{0} \Rightarrow \bar{x}=0$, we have
$$
\ell(\lambda|\widetilde{0}) = - n \lambda
\qquad
U(\lambda|\widetilde{0}) = - n
\qquad
\mathcal{J}(\lambda|\widetilde{0}) = 0 
\qquad
\mathcal{I}_{\widetilde{X}}(\lambda) = 0
$$
which does not give a valid estimator $\hat\lambda$, for $\ell(\lambda|\widetilde{0})$ is monotonically decreasing and would reach maximum in the limiting case $\lambda\to0$, outside the parametric space $(0,\infty)$.

Otherwise, when $\bar{x}\ne0$, the ML estimator comes from
$$
\ell'(\hat\lambda|\widetilde{x}) = 0 
  \Rightarrow 
    \frac{n\bar{x}}{\hat\lambda} - n = 0
  % \Rightarrow 
  %   \frac{n\bar{x}}{\hat\lambda} = n 
  \Rightarrow 
    \hat\lambda = \bar{x} 
$$
$$
\ell''(\hat\lambda|\widetilde{x})
  = -\frac{n\bar{x}}{\hat\lambda^2}
  = -\frac{n\bar{x}}{\bar{x}^2}
  % = -\frac{n\bar{x}}{\bar{x}^2}
  = -\frac{n}{\bar{x}}
  < 0
$$

So, given $\bar{x}\ne0$, 
$$
\hat\lambda_{ML} = \bar{x}
$$



#### Bias, variance and mean square error

The expected value of $\hat\lambda$ is

$$
\begin{aligned}
\mathbb{E}(\hat\lambda)
  & = \mathbb{E}(\bar{X}) 
    = \mathbb{E}\left(\frac{1}{n}\sum_{i=1}^n X_i\right) 
    = \frac{1}{n}\sum_{i=1}^n \mathbb{E}(X_i) 
    = \frac{1}{n}n \lambda 
    = \lambda \\
\end{aligned}
$$
so the ML estimator is unbiased for $\lambda$
$$
\begin{aligned}
B(\hat\lambda, \lambda)
  & = (\mathbb{E}(\hat\lambda) - \lambda)^2
    = (\lambda - \lambda)^2 
    = 0
\end{aligned}
$$
and its variance is
$$
\begin{aligned}
Var(\hat\lambda) 
  = Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right)
  = \frac{1}{n^2}\sum_{i=1}^n Var\left(X_i\right)
  = \frac{1}{n^2} n\lambda
  = \frac{\lambda}{n}
\end{aligned}
$$
so that its mean square error is
$$
\begin{aligned}
MSE(\hat\lambda, \lambda)
  & = \mathbb{E}\left( (\hat\lambda - \lambda)^2  \right) 
    = Var(\hat\lambda) + B(\hat\lambda, \lambda)
    = \frac{\lambda}{n} + 0 
    = \frac{\lambda}{n} 
\end{aligned}
$$



### Moments and method of moments estimation

It is straightforward to get $\hat\lambda_{MM}$.

For one parameter, the first moment will be enough for estimation, so we will solve the equation derived from
$$
\mu_k(\hat\lambda) = m_k = \frac{1}{n}\sum_{i=1}^n x_i^k
$$
with only $k=1$.

Since $\mu_1(\lambda) = \lambda$ and $m_1 = \bar{x}$, then $\hat\lambda_{MM} = \bar{x}$.



#### Bias, variance and mean square error

Since $\hat\lambda_{MM}=\hat\lambda_{ML}$, its expected value, bias, variance and mean square error are the same of the ML estimator.




### Least square estimation

Also straightforward.

For an i.i.d. sample $\widetilde{X}=(X_1,\ldots,X_n)$, the loss function $SS(\ell)$ for the choice of a value $\ell$ for $\hat\lambda$ is
$$
SS(\ell) = \sum_{i=1}^n (x_i - \mu_1(\ell))^2 = \sum_{i=1}^n (x_i - \ell)^2
$$
whose derivative $SS'(\ell)$ is
$$
SS'(\ell) = \sum_{i=1}^n -2(x_i - \ell)
$$
which, when equaled to zero, $SS'(\ell^*) = 0$ gives
$$
\sum_{i=1}^n -2(x_i - \ell^*) = 0 
  \Rightarrow
    n\ell^* = \sum_{i=1}^n x_i
$$
therefore
$$
\hat\lambda_{LS} = \bar{x}.
$$

#### Bias, variance and mean square error

Since $\hat\lambda_{LS}=\hat\lambda_{MM}=\hat\lambda_{ML}$, its expected value, bias, variance and mean square error are the same of ML and MM estimators.



<!-- ----------------------------------------------------------------------- -->

## The zero-truncated Poisson model

### Log-likelihood and maximum likelihood estimation

The log-likelihood function $\ell(\lambda|y)$, the score function $U(\lambda|y)$, observed information $\mathcal{J}(\lambda|y)$, and Fisher information $\mathcal{I}_Y(\lambda)$ of a single observation $Y \sim ZTruncPoi(\lambda)$ are
$$
\begin{aligned}
\ell(\lambda|y) 
  & = \log(P(Y=y)) 
  % & = \log(P(Y=y)) \\
  % & = \log\left(\frac{\lambda^y}{(e^\lambda - 1) y!}\right) \\
  % & = \log(\lambda^y) - \log(e^\lambda - 1) - \log(y!) \\
  % & = y\log(\lambda) - \log(e^\lambda - 1) \\
  = y\log(\lambda) - \log(e^\lambda-1) \\
%
U(\lambda|y) 
  & = \ell'(\lambda|y)
  % & = \ell'(\lambda|y) \\
  % & = \frac{y}{\lambda} - \frac{d}{d\lambda}\log(e^\lambda-1) \\
  % & = \frac{y}{\lambda} - \frac{1}{e^\lambda-1}\frac{d}{d\lambda}(e^\lambda-1) \\
  % & = \frac{y}{\lambda} - \frac{1}{e^\lambda-1}e^\lambda \\
  % & = \frac{y}{\lambda} - \frac{1}{1-e^{-\lambda}} \\
  = \frac{y}{\lambda} - \frac{1}{1-e^{-\lambda}} \\
%
\mathcal{J}(\lambda|y) 
  & = - U'(\lambda|y) 
  % & = - U'(\lambda|y) \\
  % & = \frac{d}{d\lambda} \left(\frac{y}{\lambda} - \frac{1}{1-e^{-\lambda}}\right) \\
  % & = \frac{d}{d\lambda}\frac{y}{\lambda} - \frac{d}{d\lambda}\frac{1}{1-e^{-\lambda}} \\
  % & = -\frac{y}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2} \\
  = -\frac{y}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2} \\
%
\mathcal{I}_X(\lambda) 
  & = \mathbb{E}_Y(\mathcal{J}(\lambda|Y))
  % & = \mathbb{E}_Y(\mathcal{J}(\lambda|Y)) \\
  % & = \mathbb{E}_Y\left(-\frac{y}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2}\right) \\
  % & = -\frac{\mathbb{E}_Y(y)}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2} \\
  % & = -\frac{\lambda}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2} \\
  % & = -\frac{1}{\lambda} + \frac{4}{\sinh(\lambda/2)^2} \\
  = -\frac{1}{\lambda} + \frac{4}{\sinh(\lambda/2)^2} \\
\end{aligned}
$$
unless a term constant with respect to $\lambda$.

For an i.i.d. sample $\widetilde{Y}=(Y_1,\ldots,Y_n)$, the log-likelihood function $\ell(\lambda|\widetilde{y})$, the score function $U(\lambda|\widetilde{y})$, observed information $\mathcal{J}(\lambda|\widetilde{y})$, and Fisher information $\mathcal{I}_{\widetilde{Y}}(\lambda)$ are
$$
\begin{aligned}
\ell(\lambda|\widetilde{y}) 
  & = \sum_{i=1}^n \ell(\lambda|y_i) 
  % & = \sum_{i=1}^n \ell(\lambda|y_i) \\
  % & = \sum_{i=1}^n \left(y_i\log(\lambda) - \log(e^\lambda-1)\right) \\
  % & = \log(\lambda) \sum_{i=1}^n y_i - n \log(e^\lambda-1) \\
  % & = n \bar{y} \log(\lambda) - n \log(e^\lambda-1) \\
  = n \bar{y} \log(\lambda) - n \log(e^\lambda-1) \\
%
U(\lambda|\widetilde{y}) 
  & = \sum_{i=1}^n U(\lambda|y_i)
  % & = \sum_{i=1}^n U(\lambda|y_i) \\
  % & = \sum_{i=1}^n \left(\frac{y_i}{\lambda} - \frac{1}{1-e^{-\lambda}}\right) \\
  % & = \frac{\sum_{i=1}^n y_i}{\lambda} - \frac{n}{1-e^{-\lambda}} \\
  % & = \frac{n \bar{y}}{\lambda} - \frac{n}{1-e^{-\lambda}} \\
  = \frac{n \bar{y}}{\lambda} - \frac{n}{1-e^{-\lambda}} \\
%
\mathcal{J}(\lambda|\widetilde{y}) 
  & = \sum_{i=1}^n - U'(\lambda|y_i)
  % & = \sum_{i=1}^n - U'(\lambda|y_i) \\
  % & = \sum_{i=1}^n \left(-\frac{y_i}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2}\right) \\
  % & = -\frac{\sum_{i=1}^n y_i}{\lambda^2} + \frac{4n}{\sinh(\lambda/2)^2} \\
  % & = -\frac{n \bar{y}}{\lambda^2} + \frac{4n}{\sinh(\lambda/2)^2} \\
  = -\frac{n \bar{y}}{\lambda^2} + \frac{4n}{\sinh(\lambda/2)^2} \\
%
\mathcal{I}_{\widetilde{Y}}(\lambda) 
  & = n \mathcal{I}_Y(\lambda)
  % & = n \mathcal{I}_Y(\lambda) \\
  % & = n \cdot \left(-\frac{1}{\lambda} + \frac{4}{\sinh(\lambda/2)^2}\right)\\
  % & = -\frac{n}{\lambda} + \frac{4}{\sinh(\lambda/2)^2} \\
  = -\frac{n}{\lambda} + \frac{4}{\sinh(\lambda/2)^2} \\
\end{aligned}
$$
by the additivity of the likelihood function of i.i.d. samples.

Since zeroes are excludes from the sample space, the worst case scenario from the regular Poisson model cannot occur, so the ML estimator comes from
$$
\ell'(\hat\lambda|\widetilde{y}) = 0
  \Rightarrow
    \frac{n \bar{y}}{\hat\lambda} - \frac{n}{1-e^{-\hat\lambda}} = 0
  \Rightarrow
    \frac{\hat\lambda}{\bar{y}} = 1-e^{-\hat\lambda}
$$
which is an equation in the form $ax=1-e^{-x}$ for $x>0$ and $a\le1$.

```{r}
#| label: fig-trunc-poi

ggplot() +
  geom_function(fun = \(x) 1 - exp(-x), color = "black") +
  geom_function(fun = \(x) x / 1.3, color = "green") +
  geom_function(fun = \(x) x , color = "red") +
  xlim(0,1) +
  coord_fixed() +
  ggtitle(
    TeX("Behavior of $1-e^{-x}$ against $ax$"),
    subtitle = TeX("for $a<1$ (green) and $a=1$ (red)")
  ) +
  theme_classic()
```

As can be seen, there is a worst case scenario for the zero-truncated Poisson: when $\bar{y}=1$.

When $\bar{y}=1$, the equation $ax=1-e^{-x}$ becomes $x=1-e^{-x}$, whose only solutions would be $x=0$ (since $x>1-e^{-x}$ for $x\ne0$), which, back in the estimation problem, would mean the solution for $\hat\lambda = 1-e^{-\hat\lambda}$ would lie outside the parametric space.

Granted $\bar{y}>1$, from the behavior of $1-e^{-x}$ and $ax$ with $a\in(0,1)$, we can assure the existence and unicity of the maximum of $\ell(\lambda|\widetilde{y})$, so that the maximum likelihood estimator $\hat\lambda_{LM}$ can be found by numerically solving 
$$
\frac{\hat\lambda}{\bar{y}} = 1-e^{-\hat\lambda}
$$

For an analytic solution, we address the reader to the @sec-analytic-solution-ML, where we establish that
$$
\hat\lambda_{ML} = \bar{y} + W_0(-\bar{y}e^{-\bar{y}})
$$
where $W_0$ is the main branch of Lambert function $W_k$, which plays the role of a bias correction term, since all zero observations (that would be present were $Y$ not zero-truncated) were excluded before taking the average $\bar{y}$.

If available, the use of $W_0$ frees us from using numeric solution.




$$
$$
<!-- ----------------------------------------------------------------------- -->
<!-- ----------------------------------------------------------------------- -->


<!-- ----------------------------------------------------------------------- -->

## Appendix 

### The Lambert function $W_0(x)$ {#sec-Lambert-W0}

The general multibranch $W_k(z)$ is meant to revert $y=xe^x$ on the complex domain, so that every element $z$ of the set $S(y)=\bigcup_{k\in\mathbb{Z}}\{W_k(y)\}$ has the property $ze^z=y$ for every $y\in\mathbb{C}$.

Constrained to the real domain, we can say about the function $y=xe^x$ that:

- its first derivative is $y'=(1+x)e^x$
- its second derivative is $y''=(2+x)e^x$
- since $e^x>0$ for $x\in\mathbb{R}$, then 
    - $y \lesseqgtr 0$ implies $x \lesseqgtr 0$, so
        - $xe^x$ is negative for $x<0$,
        - $xe^x$ is zero at $x=0$, and
        - $xe^x$ is positive for $x>0$;
    - $y' \lesseqgtr -1$ implies $x \lesseqgtr -1$, so
        - $xe^x$ is monotonically decreasing for $x< -1$,
        - $xe^x$ reaches minimum $-1/e\approx$`r -exp(-1)` at $x=-1$, and
        - $xe^x$ is monotonically increasing for $x> -1$;
    - $y'' \lesseqgtr -2$ implies $x \lesseqgtr -2$, so
        - $xe^x$ is convex for $x< -2$,
        - $xe^x$ is an inflexion point at $x=0$, and
        - $xe^x$ is concave for $x> -2$.

The branch $W_0$ of the multibranch function $W_k$ restricted to the real domain play the role of the inverse of the function $y=xe^x$ over $x\in(-1,\infty)$, which renders $y\in(-1/e,\infty)$.

```{r}
#| label: fig-functions-exp-xexp
#| fig-cap: "Function $e^x$ and $xe^x$"

ggplot() +
  geom_function(fun = exp, color = "black") +
  geom_function(fun = \(x) x*exp(x), color = "blue") +
  ggtitle(
    TeX("Comparing $e^x$ and $xe^x$"),
    subtitle = TeX("$e^x$ (black) and $xe^x$ (blue)")
  ) +
  xlim(-2,2) +
  theme_classic()
```

```{r}
#| label: fig-functions-log-W0
#| fig-cap: "Function $\\log(x)$ and $W_0(x)$"
#| warning: false

ggplot() +
  geom_function(fun = log, color = "black") +
  geom_function(fun = lambertW0, color = "blue") +
  xlim(-1,5) +
  ggtitle(
    TeX("Comparing $\\log(x)$ and $W_0(x)$"),
    subtitle = TeX("$\\log(x)$ (black) and $W_0(x)")
  ) +
  theme_classic()
```

```{r}
#| label: fig-functions-W0-negat
#| fig-cap: "Functions $W_0(x)$ for $x> -1/e$"
#| warning: false

ggplot() +
  geom_function(fun = lambertW0) +
  xlim(-exp(-1),1) +
  ggtitle(TeX("Function $W_0(x)$ for $x> -1/e$")) +
  theme_classic()
```



### Zero-truncated Poisson model

#### Derivatives used to study the log-likelihood

$$
\begin{aligned}
\frac{d}{d\lambda}\log(e^\lambda-1) 
  & = \frac{1}{e^\lambda-1}\frac{d}{d\lambda}(e^\lambda-1) 
    = \frac{1}{e^\lambda-1}e^\lambda 
    = \frac{1}{1-e^{-\lambda}}\\
\frac{d}{d\lambda} \frac{1}{1-e^{-\lambda}}
  & =  \frac{d}{d\lambda} (1-e^{-\lambda})^{-1}   
    =  (-1)(1-e^{-\lambda})^{-2} \frac{d}{d\lambda} (1-e^{-\lambda}) \\
  & =  -\frac{1}{(1-e^{-\lambda})^2} \frac{d}{d\lambda}(-e^{-\lambda})   
    =  \frac{1}{(1-e^{-\lambda})^2} \frac{d}{d\lambda} e^{-\lambda} \\
  & =  \frac{1}{(1-e^{-\lambda})^2} e^{-\lambda}\frac{d}{d\lambda} (-\lambda)   
    =  \frac{e^{-\lambda}}{(1-e^{-\lambda})^2} (-1) \\
  & =  -\frac{e^{-\lambda}}{(1-e^{-\lambda})^2}    
    =  -\frac{(e^{-\lambda/2})^2}{(1-e^{-\lambda})^2}    
    =  -\left(\frac{e^{-\lambda/2}}{1-e^{-\lambda}}\right)^2 \\
  & = -\left(\frac{e^{-\lambda/2}}{1-e^{-\lambda}}\right)^2   
    =  -\left(\frac{e^{-\lambda/2}e^{\lambda/2}}{(1-e^{-\lambda})e^{\lambda/2}}\right)^2  \\
  & =  -\left(\frac{1}{e^{\lambda/2}-e^{-\lambda+\lambda/2}}\right)^2   
    =  -\left(\frac{1}{e^{\lambda/2}-e^{-\lambda/2}}\right)^2  \\
  & =  -\frac{1}{(e^{\lambda/2}-e^{-\lambda/2})^2}    
    =  -\frac{1}{(2\sinh(\lambda/2))^2}  \\
  & =  -\frac{4}{\sinh(\lambda/2)^2}  \\
\end{aligned}
$$



#### Analytical solution of the ML estimator {#sec-analytic-solution-ML}

Back to the estimation problem, the Lambert function $W_0(x)$ presented in @sec-Lambert-W0 will be useful after some transformation in original estimation equation.

Consider the estimation equation
$$\frac{\hat\lambda}{\bar{y}} = 1-e^{-\hat\lambda}$$
in the form
$$\frac{x}{b} = 1-e^{-x}$$

Some assembly is required
$$
\frac{x}{b} = 1 - e^{-x} \Leftrightarrow
e^{-x} = 1 - \frac{x}{b} \Leftrightarrow
1 = \left(1 - \frac{x}{b}\right) e^x,
$$
now changing variable $x$ for $u=1-x/b$, which gives $x=b(1-u)$ and
$$
1 = \left(1 - \frac{x}{b}\right) e^x = ue^{b(1-u)}=ue^be^{-bu}
$$
$$
e^{-b}=ue^{-bu} \Leftrightarrow -be^{-b}=(-bu)e^{-bu}
$$

Since $\bar{y}>1$, we can establish $-1/e < -\bar{y}e^{-\bar{y}} < 0$, so we will deal with $-1/e < -be^{-b} < 0$ and, therefore, we will use $W_0(x)$ to develop further the equation:
$$
W_0(-be^{-b})=W_0((-bu)e^{-bu})=-bu=-b\cdot\left(1-\frac{x}{b}\right)=-b+x
$$
$$
x = b + W_0(-be^{-b})
$$
which, back to the estimation problem, gives
$$
\hat\lambda = \bar{y} + W_0(-\bar{y}e^{-\bar{y}}).
$$

Since $\bar{y}>1$, we have $-\bar{y}e^{-\bar{y}}<0$, which gives $W_0(-\bar{y}e^{-\bar{y}})<0$, so that $\hat\lambda$ is $\bar{y}$ minus a bias correction term that takes into account the fact that all zero observations were excluded before taking the average $\bar{y}$.



#### Analytical solution of the MM estimator {#sec-analytic-solution-MM}

The estimation equation 
$$
(\hat\lambda - \bar{y}) e^{\hat\lambda} = - \bar{y} + 1
$$
becomes, after changing variables $u=\hat\lambda - \bar{y}$ and $\hat\lambda = u + \bar{y}$, 
$$
\begin{aligned}
ue^{u + \bar{y}} & = ue^u e^{\bar{y}} = - \bar{y} + 1\\
ue^u & = - (\bar{y} - 1)e^{-\bar{y}}.
\end{aligned}
$$

Once granted $\bar{y}>1$, $-(\bar{y} - 1)e^{-\bar{y}} < 0$, we will have 
$$
\bar{y} > 1  \Rightarrow \bar{y} - 1 > 0 \Rightarrow -(\bar{y} - 1) < 0 \Rightarrow -(\bar{y} - 1)e^{-\bar{y}} < 0
$$
on one hand and, on the other hand, 
$$
-(\bar{y} - 1)e^{-\bar{y}} = -\bar{y}e^{-\bar{y}} + e^{-\bar{y}} > -\bar{y}e^{-\bar{y}} > -1/e,
$$
therefore $-1/e < -(\bar{y} - 1)e^{-\bar{y}} < 0$ and we can use once again the Lambert function $W_0$ to find $\hat\lambda$:
$$
\begin{aligned}
W_0(ue^u) & = W_0(- (\bar{y} - 1)e^{-\bar{y}})\\
u & = \hat\lambda - \bar{y} = W_0(- (\bar{y} - 1)e^{-\bar{y}})\\
\hat\lambda & = \bar{y} + W_0(- (\bar{y} - 1)e^{-\bar{y}})\\
\end{aligned}
$$



