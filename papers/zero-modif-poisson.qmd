---
title: "A Study on the Zero-Modified Poisson Model"
author: "Marcelo Ventura Freire"
format: 
  # pdf
  html:
    embed-resources: true
    toc: true
    toc-depth: 4
    number-sections: true
execute: 
  echo: false
  cache: true
---

```{r}
#| label: setup
#| include: false

library(tidyverse)  # my go-to package
library(actuar)     # dzmpois() 
library(latex2exp)  # TeX()
library(patchwork)
library(lamW)

`%$%` <- paste0  # concatenação de string
```



## Introduction

I did not invented this model, but I will give the proper credits later, when I write a proper introduction.

### Literature review

Soon...

### Examples of applications

TO-DO: examples of phenomena with occurrences of over- and under-occurrences of zero observations.



## Definitions

From the (regular) Poisson distribution $X \sim Poi(\lambda)$ with $\lambda\in(0,\infty)$ and $x=0,1,2,\ldots$
$$
P(X=x) = \frac{e^{-\lambda}\lambda^x}{x!}
$$
and $p_0=P(X=0)=e^{-\lambda}$, we define:

- the *zero-truncated* Poisson distribution $Y \sim ZTruncPoi(\lambda)$, with 
  $\lambda\in(0,\infty)$
  $$
  P(Y=y) 
    = P(X=y|X>0) 
    = \frac{P(X=y)}{1-P(X=0)}
    % = \frac{\frac{e^{-\lambda}\lambda^y}{y!}}{(1-e^{-\lambda})}
    = \frac{\lambda^y}{(e^\lambda-1)y!},
  $$
  for $y=1,2,\ldots$
- the *zero-inflated* Poisson distribution $Z \sim ZInflPoi(\pi, \lambda)$, 
  with $\pi\in(0,1)$ and $\lambda\in(0,\infty)$
  $$
  P(Z=z) =
    \begin{cases}
    \pi + (1-\pi) \cdot P(X=0) 
      = \pi + (1-\pi)e^{-\lambda}
      & z=0\\
    (1-\pi) P(X=z) 
      = \frac{(1-\pi)e^{-\lambda}\lambda^z}{z!},
      & z=1,2,\ldots
    \end{cases}
  $$
  which can be thought of as a mixture model as 
  $Z = I \times 0 + (1 - I) \times X$
  where $I \sim Bern(\pi)$ independent of $X$.
- the (general) *zero-modified* Poisson distribution
  $W \sim ZModPoi(p, \lambda)$, with $p\in(0,1)$ and $\lambda>0$
  $$
  P(W=w) =
    \begin{cases}
    p & w=0\\
    (1-p) \cdot P(Y=w) 
      = \frac{(1-p)\lambda^w}{(e^\lambda-1)w!},
      & w=1,2,\ldots
    \end{cases}
  $$
  which can be thought of as a hurdle model as 
  $W = J \times 0 + (1 - J) \times Y$
  where $J \sim Bern(p)$ independent of $Y$.


Note that the regular, the zero-inflated, the zero-deflated, and the zero-truncated Poisson distributions can be obtained from the general model respectively when $p=p_0$, $p>p_0$, $p<p_0$, and $p\to0$.

```{r}
#| label: fig-1st-parametrization
#| fig-cap: "Visualizing difference among distributions"

xfinal <- 6
expand_grid(
  x = 0:xfinal,
  lambda = c(0.5, 1, 2, 3)
) |> 
  mutate(
    RegPoi =  
      dzmpois(x, lambda, dpois(0, lambda)),
    ZTruncPoi =
      dzmpois(x, lambda, 0),
    ZDeflPoi = 
      dzmpois(x, lambda, 0.5 * dpois(0, lambda)),
    ZInfPoi = 
      dzmpois(x, lambda, 1.5 * dpois(0, lambda)),
  ) |> 
  pivot_longer(
    cols = RegPoi:ZInfPoi,
    values_to = "y",
    names_to = "distrib",
  ) |> 
  ggplot() +
  aes(x = x, y = y, group = distrib, color = distrib) +
  geom_point() +
  geom_line() +
  scale_x_continuous(
    breaks = 0:xfinal,
    labels = scales::label_number(accuracy = 1)
  ) +
  ylim(0:1) +
  facet_wrap(~ lambda, labeller = "label_both") +
  ggtitle( 
    "Comparing regular, zero-truncated, zero-deflated, and zero-inflated models" ,
    subtitle = 
      "Regular ($p=p_0$), zero-truncated ($p=0$), " %$%
      "zero-deflated ($p<p_0$), and zero-inflated ($p>p_0$)" |> 
      TeX()
  ) +
  xlab("z") +
  ylab("P(Z=z)") +
  theme_classic()
```


A second parametrization for the zero-modified Poisson distribution can be obtained by setting $p=e^{-\lambda+\theta}$, so that, for $Z \sim ZModPoi(\theta, \lambda)$, we have
$$
P(W=w) = 
\begin{cases}
e^{-\lambda+\theta}, 
  & w=0\\
\frac{(1-e^{-\lambda+\theta})\lambda^w}{(e^{\lambda}-1)w!}
  & w=1,2,\ldots
\end{cases}
$$
in which case, regular, zero-inflated, and zero-deflated Poisson can be obtained when $\theta=0$, $\theta>0$, and $\theta<0$. However, a constraint must be set on $\theta$ so that $0<p<1$:
$$
e^{-\lambda+\theta}<1
  \Leftrightarrow
    -\lambda+\theta<0
  \Leftrightarrow
    \theta<\lambda.
$$
This way, we can refer to $Z \sim ZModPoi(\theta,\lambda)$ for $\lambda>0$ and $\theta<\lambda$ as the model in its second parametrization.


For algebraic ease, we may use a third parametrization setting $1-p=e^\gamma$, subject to $0<e^\gamma<1 \Leftrightarrow \gamma<0$, 
$$
P(W=w) = 
\begin{cases}
1-e^{\gamma}, 
  & y=0\\
\frac{e^\gamma\lambda^w}{(e^{\lambda}-1)w!}
  & w=1,2,\ldots
\end{cases}
$$
so that the parametrization used in each context will be defined by the use of the three different parameters.



## The regular Poisson model

### Log-likelihood and maximum likelihood estimation

The log-likelihood function $\ell(\lambda|x)$, the score function $U(\lambda|x)$, observed information $\mathcal{J}(\lambda|x)$, and Fisher information $\mathcal{I}_X(\lambda)$ of a single observation $X \sim Poi(\lambda)$ are
$$
\begin{align}
\ell(\lambda|x) 
  & = \log(P(X=x)) 
  % & = \log(P(X=x)) \\
  % & = -\lambda + x \log(\lambda) \\
  % & = x \log(\lambda) - \lambda \\
  = x \log(\lambda) - \lambda \\
%
U(\lambda|x) 
  & = \ell'(\lambda|x)
  % & = \ell'(\lambda|x) \\
  % & = \frac{x}{\lambda} - 1 \\
  = \frac{x}{\lambda} - 1 \\
%
\mathcal{J}(\lambda|x) 
  & = - U'(\lambda|x) 
  % & = - U'(\lambda|x) \\
  % & = - \ell''(\lambda|x) \\
  % & = \frac{x}{\lambda^2} \\
  = \frac{x}{\lambda^2} \\
%
\mathcal{I}_X(\lambda) 
  & = \mathbb{E}_X(\mathcal{J}(\lambda|X)) 
  % & = \mathbb{E}_X(\mathcal{J}(\lambda|X)) \\
  % & = \mathbb{E}\left(\frac{X}{\lambda^2}\right) \\
  % & = \frac{\lambda}{\lambda^2} \\
  % & = \frac{1}{\lambda}
  = \frac{1}{\lambda}
\end{align}
$$
unless a term constant with respect to $\lambda$.


For an i.i.d. sample $\widetilde{X}=(X_1,\ldots,X_n)$, the log-likelihood function $\ell(\lambda|\widetilde{x})$, the score function $U(\lambda|\widetilde{x})$, observed information $\mathcal{J}(\lambda|\widetilde{x})$, and Fisher information $\mathcal{I}_{\widetilde{X}}(\lambda)$ are
$$
\begin{align}
\ell(\lambda|\widetilde{x}) 
  & = \sum_{i=1}^n \ell(\lambda|x_i)
  % & = \sum_{i=1}^n \ell(\lambda|x_i) \\
  % & = \sum_{i=1}^n -\lambda + x_i \log(\lambda) 
  % & = - n \lambda + \log(\lambda) \sum_{i=1}^n x_i
  % & = - n \lambda + s \log(\lambda)
  % & = - n \lambda + n\bar{x} \log(\lambda) \\
  % & = n \bar{x} \log(\lambda) - n \lambda \\
  = n \bar{x} \log(\lambda) - n \lambda \\
%
U(\lambda|\widetilde{x}) 
  & = \ell'(\lambda|\widetilde{x}) 
  % & = \ell'(\lambda|\widetilde{x}) \\
  % & = \frac{n\bar{x}}{\lambda} - n \\
    = \frac{n\bar{x}}{\lambda} - n \\
%
\mathcal{J}(\lambda|\widetilde{x}) 
  & = - U'(\lambda|\widetilde{x}) 
  % & = - U'(\lambda|\widetilde{x}) \\
  % &  = \frac{n\bar{x}}{\lambda^2} \\
  = \frac{n\bar{x}}{\lambda^2} \\
%
\mathcal{I}_{\widetilde{X}}(\lambda) 
  & = n \cdot \mathcal{I}_X(\lambda) 
  % & = n \cdot \mathcal{I}_X(\lambda)  \\
  % & = \frac{n}{\lambda} \\
    = \frac{n}{\lambda}
\end{align}
$$
by the additivity of the likelihood function of i.i.d. samples.

Note that, in the worst case scenario $\widetilde{x}=\widetilde{0} \Rightarrow \bar{x}=0$, we have
$$
\ell(\lambda|\widetilde{0}) = - n \lambda
\qquad
U(\lambda|\widetilde{0}) = - n
\qquad
\mathcal{J}(\lambda|\widetilde{0}) = 0 
\qquad
\mathcal{I}_{\widetilde{X}}(\lambda) = 0
$$
which does not give a valid estimator $\hat\lambda$, for $\ell(\lambda|\widetilde{0})$ is monotonically decreasing and would reach maximum in the limiting case $\lambda\to0$, outside the parametric space $(0,\infty)$.

Otherwise, when $\bar{x}\ne0$, the ML estimator comes from
$$
\ell'(\hat\lambda|\widetilde{x}) = 0 
  \Rightarrow 
    \frac{n\bar{x}}{\hat\lambda} - n = 0
  % \Rightarrow 
  %   \frac{n\bar{x}}{\hat\lambda} = n 
  \Rightarrow 
    \hat\lambda = \bar{x} 
$$
$$
\ell''(\hat\lambda|\widetilde{x})
  = -\frac{n\bar{x}}{\hat\lambda^2}
  = -\frac{n\bar{x}}{\bar{x}^2}
  % = -\frac{n\bar{x}}{\bar{x}^2}
  = -\frac{n}{\bar{x}}
  < 0
$$

So, given $\bar{x}\ne0$, 
$$
\hat\lambda_{ML} = \bar{x}
$$


### Moments and method of moments estimation

It is straightforward to get $\hat\lambda_{MM}$.

For one parameter, the first moment will be enough for estimation.

Since 
$$
\mu_1 = \mathbb{E}(X^1) = \lambda,
$$
then 
$$
\frac{1}{n} \sum_{i=1}^n x_i^1 = \hat\lambda_1,
$$
so
$$
\hat\lambda_{MM} = \bar{x}.
$$



### Least square estimation

Also straightforward.

For an i.i.d. sample $\widetilde{X}=(X_1,\ldots,X_n)$, the loss function $SS(\ell)$ for the choice of a value $\ell$ for $\hat\lambda$ is
$$
SS(\ell) = \sum_{i=1}^n (x_i - \ell)^2
$$
whose derivative $SS'(\ell)$ is
$$
SS'(\ell) = \sum_{i=1}^n -2(x_i - \ell)
$$
which, when equaled to zero, $SS'(\ell^*) = 0$ gives
$$
\sum_{i=1}^n -2(x_i - \ell^*) = 0 
  \Rightarrow
    n\ell^* = \sum_{i=1}^n x_i
$$
therefore
$$
\hat\lambda_{LS} = \bar{x}.
$$



## The zero-truncated Poisson model

### Log-likelihood and maximum likelihood estimation

The log-likelihood function $\ell(\lambda|y)$, the score function $U(\lambda|y)$, observed information $\mathcal{J}(\lambda|y)$, and Fisher information $\mathcal{I}_Y(\lambda)$ of a single observation $Y \sim ZTruncPoi(\lambda)$ are
$$
\begin{align}
\ell(\lambda|y) 
  & = \log(P(Y=y)) 
  % & = \log(P(Y=y)) \\
  % & = \log\left(\frac{\lambda^y}{(e^\lambda - 1) y!}\right) \\
  % & = \log(\lambda^y) - \log(e^\lambda - 1) - \log(y!) \\
  % & = y\log(\lambda) - \log(e^\lambda - 1) \\
  = y\log(\lambda) - \log(e^\lambda-1) \\
%
U(\lambda|y) 
  & = \ell'(\lambda|y)
  % & = \ell'(\lambda|y) \\
  % & = \frac{y}{\lambda} - \frac{d}{d\lambda}\log(e^\lambda-1) \\
  % & = \frac{y}{\lambda} - \frac{1}{e^\lambda-1}\frac{d}{d\lambda}(e^\lambda-1) \\
  % & = \frac{y}{\lambda} - \frac{1}{e^\lambda-1}e^\lambda \\
  % & = \frac{y}{\lambda} - \frac{1}{1-e^{-\lambda}} \\
  = \frac{y}{\lambda} - \frac{1}{1-e^{-\lambda}} \\
%
\mathcal{J}(\lambda|y) 
  & = - U'(\lambda|y) 
  % & = - U'(\lambda|y) \\
  % & = \frac{d}{d\lambda} \left(\frac{y}{\lambda} - \frac{1}{1-e^{-\lambda}}\right) \\
  % & = \frac{d}{d\lambda}\frac{y}{\lambda} - \frac{d}{d\lambda}\frac{1}{1-e^{-\lambda}} \\
  % & = -\frac{y}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2} \\
  = -\frac{y}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2} \\
%
\mathcal{I}_X(\lambda) 
  & = \mathbb{E}_Y(\mathcal{J}(\lambda|Y))
  % & = \mathbb{E}_Y(\mathcal{J}(\lambda|Y)) \\
  % & = \mathbb{E}_Y\left(-\frac{y}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2}\right) \\
  % & = -\frac{\mathbb{E}_Y(y)}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2} \\
  % & = -\frac{\lambda}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2} \\
  % & = -\frac{1}{\lambda} + \frac{4}{\sinh(\lambda/2)^2} \\
  = -\frac{1}{\lambda} + \frac{4}{\sinh(\lambda/2)^2} \\
\end{align}
$$
unless a term constant with respect to $\lambda$.

For an i.i.d. sample $\widetilde{Y}=(Y_1,\ldots,Y_n)$, the log-likelihood function $\ell(\lambda|\widetilde{y})$, the score function $U(\lambda|\widetilde{y})$, observed information $\mathcal{J}(\lambda|\widetilde{y})$, and Fisher information $\mathcal{I}_{\widetilde{Y}}(\lambda)$ are
$$
\begin{align}
\ell(\lambda|\widetilde{y}) 
  & = \sum_{i=1}^n \ell(\lambda|y_i) 
  % & = \sum_{i=1}^n \ell(\lambda|y_i) \\
  % & = \sum_{i=1}^n \left(y_i\log(\lambda) - \log(e^\lambda-1)\right) \\
  % & = \log(\lambda) \sum_{i=1}^n y_i - n \log(e^\lambda-1) \\
  % & = n \bar{y} \log(\lambda) - n \log(e^\lambda-1) \\
  = n \bar{y} \log(\lambda) - n \log(e^\lambda-1) \\
%
U(\lambda|\widetilde{y}) 
  & = \sum_{i=1}^n U(\lambda|y_i)
  % & = \sum_{i=1}^n U(\lambda|y_i) \\
  % & = \sum_{i=1}^n \left(\frac{y_i}{\lambda} - \frac{1}{1-e^{-\lambda}}\right) \\
  % & = \frac{\sum_{i=1}^n y_i}{\lambda} - \frac{n}{1-e^{-\lambda}} \\
  % & = \frac{n \bar{y}}{\lambda} - \frac{n}{1-e^{-\lambda}} \\
  = \frac{n \bar{y}}{\lambda} - \frac{n}{1-e^{-\lambda}} \\
%
\mathcal{J}(\lambda|\widetilde{y}) 
  & = \sum_{i=1}^n - U'(\lambda|y_i)
  % & = \sum_{i=1}^n - U'(\lambda|y_i) \\
  % & = \sum_{i=1}^n \left(-\frac{y_i}{\lambda^2} + \frac{4}{\sinh(\lambda/2)^2}\right) \\
  % & = -\frac{\sum_{i=1}^n y_i}{\lambda^2} + \frac{4n}{\sinh(\lambda/2)^2} \\
  % & = -\frac{n \bar{y}}{\lambda^2} + \frac{4n}{\sinh(\lambda/2)^2} \\
  = -\frac{n \bar{y}}{\lambda^2} + \frac{4n}{\sinh(\lambda/2)^2} \\
%
\mathcal{I}_{\widetilde{Y}}(\lambda) 
  & = n \mathcal{I}_Y(\lambda)
  % & = n \mathcal{I}_Y(\lambda) \\
  % & = n \cdot \left(-\frac{1}{\lambda} + \frac{4}{\sinh(\lambda/2)^2}\right)\\
  % & = -\frac{n}{\lambda} + \frac{4}{\sinh(\lambda/2)^2} \\
  = -\frac{n}{\lambda} + \frac{4}{\sinh(\lambda/2)^2} \\
\end{align}
$$
by the additivity of the likelihood function of i.i.d. samples.

Since zeroes are excludes from the sample space, the worst case scenario from the regular Poisson model cannot occur, so the ML estimator comes from
$$
\ell'(\hat\lambda|\widetilde{y}) = 0
  \Rightarrow
    \frac{n \bar{y}}{\hat\lambda} - \frac{n}{1-e^{-\hat\lambda}} = 0
  \Rightarrow
    \frac{\hat\lambda}{\bar{y}} = 1-e^{-\hat\lambda}
$$
which is an equation in the form $ax=1-e^{-x}$ for $x>0$ and $a\le1$.

```{r}
#| label: fig-trunc-poi

ggplot() +
  geom_function(fun = \(x) 1 - exp(-x), color = "black") +
  geom_function(fun = \(x) x / 1.3, color = "green") +
  geom_function(fun = \(x) x , color = "red") +
  xlim(0,1) +
  coord_fixed() +
  ggtitle(
    TeX("Behavior of $1-e^{-x}$ against $ax$"),
    subtitle = TeX("for $a<1$ (green) and $a=1$ (red)")
  ) +
  theme_classic()
```
$$
\ell'(\hat\lambda|\widetilde{x}) = 0 
  \Rightarrow 
    -n + \frac{n\bar{x}}{\hat\lambda} = 0
  % \Rightarrow 
  %   \frac{n\bar{x}}{\hat\lambda} = n 
  \Rightarrow 
    \hat\lambda = \bar{x} 
$$
$$
\ell''(\hat\lambda|\widetilde{x})
  = -\frac{n\bar{x}}{\hat\lambda^2}
  = -\frac{n\bar{x}}{\bar{x}^2}
  % = -\frac{n\bar{x}}{\bar{x}^2}
  = -\frac{n}{\bar{x}}
  < 0
$$
#### Useful developments

##### Derivatives used to study the log-likelihood of $Y$

$$
\begin{align}
\frac{d}{d\lambda}\log(e^\lambda-1) 
  & = \frac{1}{e^\lambda-1}\frac{d}{d\lambda}(e^\lambda-1) 
    = \frac{1}{e^\lambda-1}e^\lambda 
    = \frac{1}{1-e^{-\lambda}}\\
\frac{d}{d\lambda} \frac{1}{1-e^{-\lambda}}
  & =  \frac{d}{d\lambda} (1-e^{-\lambda})^{-1}   
    =  (-1)(1-e^{-\lambda})^{-2} \frac{d}{d\lambda} (1-e^{-\lambda}) \\
  & =  -\frac{1}{(1-e^{-\lambda})^2} \frac{d}{d\lambda}(-e^{-\lambda})   
    =  \frac{1}{(1-e^{-\lambda})^2} \frac{d}{d\lambda} e^{-\lambda} \\
  & =  \frac{1}{(1-e^{-\lambda})^2} e^{-\lambda}\frac{d}{d\lambda} (-\lambda)   
    =  \frac{e^{-\lambda}}{(1-e^{-\lambda})^2} (-1) \\
  & =  -\frac{e^{-\lambda}}{(1-e^{-\lambda})^2}    
    =  -\frac{(e^{-\lambda/2})^2}{(1-e^{-\lambda})^2}    
    =  -\left(\frac{e^{-\lambda/2}}{1-e^{-\lambda}}\right)^2 \\
  & = -\left(\frac{e^{-\lambda/2}}{1-e^{-\lambda}}\right)^2   
    =  -\left(\frac{e^{-\lambda/2}e^{\lambda/2}}{(1-e^{-\lambda})e^{\lambda/2}}\right)^2  \\
  & =  -\left(\frac{1}{e^{\lambda/2}-e^{-\lambda+\lambda/2}}\right)^2   
    =  -\left(\frac{1}{e^{\lambda/2}-e^{-\lambda/2}}\right)^2  \\
  & =  -\frac{1}{(e^{\lambda/2}-e^{-\lambda/2})^2}    
    =  -\frac{1}{(2\sinh(\lambda/2))^2}  \\
  & =  -\frac{4}{\sinh(\lambda/2)^2}  \\
\end{align}
$$

```{r}
#| label: fig-functions-e^x-xe^x
#| fig-cap: "Function $e^x$ and $xe^x$"

ggplot() +
  geom_function(fun = exp, color = "black") +
  geom_function(fun = \(x) x*exp(x), color = "blue") +
  ggtitle(
    TeX("Comparing $e^x$ and $xe^x$"),
    subtitle = TeX("$e^x$ (black) and $xe^x$ (blue)")
  ) +
  xlim(-2,2) +
  theme_classic()
```


```{r}
#| label: fig-functions-log-W
#| fig-cap: "Function $\\log(x)$ and $W_0(x)$"
#| warning: false

ggplot() +
  geom_function(fun = log, color = "black") +
  geom_function(fun = lambertW0, color = "blue") +
  xlim(-1,5) +
  ggtitle(
    TeX("Comparing $\\log(x)$ and $W_0(x)$"),
    subtitle = TeX("$\\log(x)$ (black) and $W_0(x)")
  ) +
  theme_classic()
```

```{r}
#| label: fig-functions-W-negat
#| fig-cap: "Functions $W_0(x)$ for $x> -1/e$"
#| warning: false

ggplot() +
  geom_function(fun = lambertW0) +
  xlim(-exp(-1),1) +
  ggtitle(TeX("Function $W_0(x)$ for $x> -1/e$")) +
  theme_classic()
```

### Moments and method of moments estimation

It is straightforward to get $\hat\lambda_{MM}$.

For one parameter, the first moment will be enough for estimation.

Since 
$$
\mu_1 = \mathbb{E}(X^1) = \lambda,
$$
then 
$$
\frac{1}{n} \sum_{i=1}^n x_i^1 = \hat\lambda_1,
$$
so
$$
\hat\lambda_{MM} = \bar{x}.
$$



### Least square estimation

Also straightforward.

For an i.i.d. sample $\widetilde{X}=(X_1,\ldots,X_n)$, the loss function $SS(\ell)$ for the choice of a value $\ell$ for $\hat\lambda$ is
$$
SS(\ell) = \sum_{i=1}^n (x_i - \ell)^2
$$
whose derivative $SS'(\ell)$ is
$$
SS'(\ell) = \sum_{i=1}^n -2(x_i - \ell)
$$
which, when equaled to zero, $SS'(\ell^*) = 0$ gives
$$
\sum_{i=1}^n -2(x_i - \ell^*) = 0 
  \Rightarrow
    n\ell^* = \sum_{i=1}^n x_i
$$
therefore
$$
\hat\lambda_{LS} = \bar{x}.
$$







## The zero-inflated Poisson model

### Log-likelihood and maximum likelihood estimation

### Least squares estimation

### Moments and method of moments estimation




## The general zero-modified Poisson model

### Moments and method of moments estimation

### Log-likelihood and maximum likelihood estimation

The log-likelihood of a single observation $Z \sim ZModPoi(p, \lambda)$ is
$$
\begin{align}
\ell(p,\lambda|0) 
  & = \log(p) \\
\ell(p,\lambda|z)
  & = \log\left(\frac{(1-p)\lambda^z}{(e^\lambda-1)z!}\right)
    = \log(1-p)+z\log(\lambda)-\log(e^\lambda-1)
\end{align}
$$
for $z=1,2,\ldots$


and for an i.i.d. sample $Z_1,Z_2,\ldots,X_n$ of $ZModPoi(p, \lambda)$, we have
$$
\begin{align}
\ell(p,\lambda|z_1,\ldots,z_n)
  & = \sum_{i=1}^n \ell(p,\lambda|z_i) \\
  & = \sum_{i \in M}^n \ell(p,\lambda|z_i) +
      \sum_{i \not\in M}^n \ell(p,\lambda|z_i) \\
  & = m \log(p) +
      \sum_{i \not\in M}^n \log(1-p)+z\log(\lambda)-\log(e^\lambda-1) \\
  
  
  
  
  & = P(Z_1=z_1,\ldots,Z_n=z_n) \\
  & = \prod_{i=1}^n P(Z=z_i) \\
  & = \prod_{i \in M} P(Z=z_i) \times
      \prod_{i \not\in M} P(Z=z_i) \\
  & = \prod_{i \in M} P(Z=0) \times
      \prod_{i \not\in M} P(Z=z_i) \\
  & = \prod_{i \in M} e^{-\lambda} \times
      \prod_{i \not\in M} P(Z=z_i) \\
\end{align}
$$
where $M$ is the set $M=\{i=1,2,\ldots,n|z_i=0\}$ of positions of the observed zeroes and $m=#M$ is its size.


